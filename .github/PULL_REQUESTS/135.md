# Memory efficient s3 delete

**yorinasub17** commented *Sep 21, 2020*

This fixes a few bugs in the s3 deletion routine:

- `cloud-nuke` can crash with OOM if there are too many objects in the bucket. This is because it loads all the objects into memory before starting to delete them. **This was fixed by combining the deletion routine with the pager.**

- No error was returned if there was an error emptying the bucket. This led to confusing error messages where it may have failed to empty the bucket, but cloud-nuke exits because it can't delete the bucket itself. **This was fixed by updating the routine to return a combined error for all the buckets**

- Tests were not checking errors in the nuke routine, so it would exit with a pass even when the nuking failed.
<br />
***


**yorinasub17** commented *Sep 24, 2020*

Feedback from slack:

> Did you change the error handling so that the app exits if it fails to delete a resource? IMHO it would be better to report the failure and continue rather than exit (old behavior)
***

**yorinasub17** commented *Sep 30, 2020*

> Did you change the error handling so that the app exits if it fails to delete a resource? IMHO it would be better to report the failure and continue rather than exit (old behavior)

I decided to handle this in a separate PR given that the current behavior of returning errors and stopping execution mirrors all the other resources. I will work on adding a separate CLI flag to ignore errors that will make it continue processing each resource even in the face of errors.

See https://github.com/gruntwork-io/cloud-nuke/pull/136
***

**yorinasub17** commented *Sep 30, 2020*

Ok I believe I addressed all the comments! This is ready for a rereview!
***

**yorinasub17** commented *Oct 1, 2020*

Thanks for review! Going to merge this in now.
***

